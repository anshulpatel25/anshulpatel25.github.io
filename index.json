[{"content":" My meetup talk discusses about conducting Chaos Engineering experiments using Litmus Chaos Framework.\nSlides Link to heading Pictures Link to heading ","description":"","tags":["chaosengineering","highavailability"],"title":"Chaos Engineering with Litmus Chaos Framework","uri":"http://www.anshulpatel.in/meetups/chaos_engineering_infostretch_meetup_3/"},{"content":"This blog provides overview on software compilation and strategies.\nWhat is Compiler? Link to heading A compiler is a computer program that translates programming language code into machine code.\nFor example: C, C++, RUST, Golang, .NET\nWhat is Interpreter? Link to heading An interpreter is a computer program that directly executes the programming language code without requiring it to be compiled. The instructions are converted into an intermedia code format called the ByteCode which is then executed by Programming Language specific virtual machine which converts the intermediate code to the machine code.\nFor example: Python, Ruby, Javascript\nWhat is Transpiler (Source-to-Source Compiler)? Link to heading A transpiler is a computer program that translates programming code of particular language into either the same programming language or different programming language.\nFor Example: Babel, HipHop\nCompilation Strategies Link to heading Ahead of Time Compilation (AOT) Link to heading An Ahead of Time compiler translates programming language code into native machine code for the targeted architecture and operating system.\nThe code is always compiled before executing it.\nAOT Characteristics Link to heading Low Runtime CPU and Memory Usage. Application/Program startup is fast. Code is required to be generated for different combination of microprocesser architecture (For example: ARM, X64_32, X86_64, RISC) and operating systems (For example: Linux, Windows, FreeBSD). Just in Time Compilation (JIT) Link to heading A Just in Time compiler translates the programming language code into native machine code while execution at runtime. Generally, the programming languages which uses JIT strategy, converts the programming language code into an intermediate format which is then interpreted by programming language specific virtual machine to machine code.\nJIT continuously analyzes the code and performs dynamic compilation of only required sections of code which could accelerate the execution.\nJIT Characteristics Link to heading JITs are usually part of Programming Language specific virtual machine, hence the same code can be directly run on programming language specific virtual machine for different combination of microprocessor architecture and operating systems. It performs adaptive and dynamic optimization at runtime to increase the execution performance. As it continuosly profiles and analyzes the code, it has extra CPU and Memory usage. Application/Program startup is relatively slow. ","description":"","tags":["performance","programming"],"title":"Compilers, Interpreters, Transpilers and Strategies","uri":"http://www.anshulpatel.in/posts/jit_v_aot/"},{"content":" This article provides introduction to Docker, how it differs from Virtual Machine and underlining technologies.\nSlides Link to heading ","description":"","tags":["containers","linux"],"title":"Docker Fundamentals","uri":"http://www.anshulpatel.in/posts/docker_fundamentals/"},{"content":" My meetup talk discusses about conducting Chaos Engineering experiments using Gremlin platform.\nSlides Link to heading Pictures Link to heading ","description":"","tags":["chaosengineering","highavailability"],"title":"Chaos Engineering with Gremlin Platform","uri":"http://www.anshulpatel.in/meetups/chaos_engineering_infostretch_meetup_2/"},{"content":" My meetup talk discusses about basics of Chaos Engineering and how to induce Chaos Engineering via Chaos Lambda.\nSlides Link to heading Pictures Link to heading ","description":"","tags":["chaosengineering","highavailability"],"title":"Chaos Engineering with Chaos Lambda","uri":"http://www.anshulpatel.in/meetups/chaos_engineering_infostretch_meetup/"},{"content":"To understand the cloud-native, we should first understand the definition of the cloud.\nNIST defines cloud computing characteristics as follows:\nRapid elasticity On-demand self-service Broad network access Resource pooling Measured service Cloud-Native applications should be the applications which are designed and implemented to use and take advantage of the above cloud computing characteristics.\nCloud-Native applications should have the following characteristics:\nImmutable Packaging \u0026 Execution Link to heading Machine Images (For eg: AMI) and Container Images (For eg: Docker Image) allows us to bake the library dependencies along with the application. This allows the development to take advantage of Rapid Elasticity. Containers can be much more efficient than VMs as multiple containers can be executed on a single VM, hence further optimizing Resource Pooling characteristics. Functions as service (For eg: Lambda) can further optimize Resource Pooling. Decoupled Configurations \u0026 Secrets Link to heading Immutability and elasticity make it difficult to add configuration as part of code, hence centralized K/V store (For eg: Hashicorp Consul) and secrets engine (For eg: Hashicorp Vault, AWS KMS) should be implemented. The application should fetch the secrets and K/V from the above engines, providing on-demand flexibility to append, update secrets and K/V. Statelessness \u0026 Statefulness Link to heading Rapid Elasticity makes it somewhat difficult to couple data with the applications. The stateful landscape is still work-in-progress and unstable. Till the time it becomes stable, applications should use external storage, queuing and search engine systems (For eg: Redis, RabbitMQ, Kafka, MySQL, Cassandra, Elasticsearch, etc). Modular Applications Link to heading Problem with monoliths was that even if we need to scale only one module ( For eg: Payment), we need to replicate/scale the whole monolith which defeats the purpose of Resource Pooling Microservices and MicroFrontends can provide better Resource Pooling by running containers/functions on the same VMs. Also scaling Microservices becomes easy as it is independent/loosely coupled and performs single functionality of the domain. (For eg: In Ecommerce domain, single functionality can be Payment, Checkout, Carts, Orders, Shipping, etc). Most of the modern-day storage systems are multi-tenant, hence multiple applications can take advantage of this multi-tenancy (Resource Pooling)characteristics. Polyglot Paradigm Link to heading Probably the best thing of using this paradigm. As dependencies are packaged along with the application and are isolated from the underlying execution environment, the development team can use their language of choice to write their modules and achieve their use case. Centralized Logging Link to heading Due to Rapid Elasticity, it can be difficult for the development team to track down the problematic machine and view its logs, as it wouldn’t be secure as well as convenient to hand out production machine access (For eg: SSH) to different development teams. Almost all major cloud providers provide a service which can ingest log or log-like data. (For eg: Cloudwatch logs, ELK) Applications should forward their logs to centralized logging service. On-demand self-service centralized logging solution can be used by development teams to securely and conveniently access the production logs. Managed via DevOps \u0026 SRE processes Link to heading Apps should use Continuous Integration Apps should use Continuous Delivery \u0026 Deployment via declarative infrastructure-as-code, configuration management. Important metrics should be Measured for monitoring and observing application. Deployment patterns such as Rolling, Blue/Green, Canary should be practiced. Blameless postmortem should be promoted during application outage to make the application more resilient and robust. Automation Link to heading All the processes, tools which are used for development, deployment and maintenance of the apps should be automated or have API(s) for automation via custom scripts. API centric Link to heading Applications use well defined lightweight API on top of common protocols (For eg: REST, gRPC) for exchanging the data. The protocols used are intentionally language independent to support Polyglot Paradigm. API centric approach also promotes Broad network access, hence same API(s) can be consumed by Mobiles, IoT devices, Servers and Development teams which then can use it for their scenarios. ","description":"","tags":["scalability","highavailability","cloud"],"title":"What are cloud-native applications?","uri":"http://www.anshulpatel.in/posts/cloud_native/"},{"content":"This article discusses the internals about how Linux calculates CPU utilization for a process.\nWhat is Real time and Process time? Link to heading Real Time: It is an actual elapsed time that we observe on the wall clock.\nProcess Time: It is the amount of CPU time used by a process.\nWhat are Jiffies? Link to heading Process Time in Linux is measured in Jiffies. Jiffies are measured in “HZ”. Jiffies value can be fetched via sysconf(_SC_CLK_TCK) system call. #!/usr/bin/env python3 import os print(os.sysconf(os.sysconf_names['SC_CLK_TCK'])) # 100 # 1 second = 100 Jiffies How to fetch Jiffies in Linux? Link to heading Total Jiffies for all CPU(s) and individual CPU can be fetched from /proc/stat. $ cat /proc/stat | grep cpu # CPU user nice system idle iowait irq softirq steal guest guest_nice cpu 365430 546 105481 2862563 1252 0 10568 0 0 0 cpu0 47076 48 15238 743800 329 0 2116 0 0 0 cpu1 43550 43 14460 303602 26 0 1584 0 0 0 cpu2 46679 39 12309 302736 21 0 1053 0 0 0 cpu3 46041 72 12112 303436 15 0 636 0 0 0 cpu4 46400 90 13997 300582 568 0 661 0 0 0 cpu5 46818 16 13045 301880 188 0 2931 0 0 0 cpu6 44507 31 12307 302683 64 0 1213 0 0 0 cpu7 44356 58 12010 303840 38 0 370 0 0 0 Jiffies for a particular process can be fetched from /proc/{pid}/stat. cat /proc/3465/stat | awk '{print $14, $15, $16, $17}' # User System User(Waited for Children) System(Waited for Children) 59175 13926 52717 10318 Total and Process CPU time (User + System) can be sampled across pre-defined time interval to obtain the utilization. For eg: If Total CPU time (User + System) across 8 cores is 450 Jiffies (sampled per second) and Process CPU time (User + System + User of Children + System of Children) is 50 Jiffies (sampled per second), then utilization in percentage can be calculated as follows:\n# Percentage = (100 * Process Jiffies)/Total CPU Jiffies (100 * 50)/450 = 11.111% Jiffies can be converted to Real Time (seconds) via (Jiffies/100) as 1 second = 100 Jiffies, it may be greater than Real Time if a Process utilizes more than one CPU or Core. Golang Snippet Link to heading This Golang snippet samples CPU utilization per second.\nOutput Link to heading $ go run main.go Enter the PID:3465 Logging CPU % for PID 3465 2.564103% 3.947368% 0.000000% 0.000000% 4.166667% 0.000000% 1.190476% 3.773585% ","description":"","tags":["linux","performance"],"title":"How Linux calculates CPU utilization","uri":"http://www.anshulpatel.in/posts/linux_cpu_percentage/"},{"content":"Recently having binged watch Air Emergency, I felt that SREs can learn many things from aviation industry.\nLike modern aircraft, modern runtimes and operating systems are sophisticated, complex and they just don’t crash or behave weirdly.\nJust as aircraft have cockpit voice recorders and black boxes, modern systems should improve logging as the overall system matures for quick and meticulous post-incident analysis.\nPilots are well aware of the importance of each subsystem of aircraft and the impact of its failure. Similarly, SREs should be well aware of the subsystems and impact of its failure. I prefer to name changes which can have a significant impact on the overall system and can lead to SLA breaches as sensitive changes.\nJust as aircraft have pre-flight, in-flight, emergency-checklist and post-flight checklist, similar checklists should be prepared and executed by SREs before performing, while performing and after performing sensitive changes on the system. SREs should also maintain an emergency-checklist which should be executed in an event if sensitive changes go wrong.\nJust as aircraft subsystems are passed through simulations, sensitive changes should be simulated before applying it to the overall system.\nJust as pilots makes decision-based on several input parameters like aircraft instrument cluster, inputs from air traffic controller, weather information, etc. SREs should also consider analyzing information from various tools before reaching the decision. If the required tooling is unavailable, they should step forward and develop those tools.\nPilots, SREs are humans and humans are flawed to make mistakes, so like Pilots, SREs should be focussed while performing any kind of sensitive changes to systems or as Spock would say it, they shouldn’t be emotionally compromised.\nJust as Transport Safety Boards amends the appropriate documents after investigating the air crash, similarly SREs should update corresponding system knowledge base, engineering practices, checklists after the postmortem and should be reviewed periodically by the peers.\nAs Murphy’s law states “Anything that can go wrong, will go wrong”. In such events, like pilots, SREs should rely on their knowledge, aptitude, and experience to solve these novel challenging situations.\n","description":"","tags":["sitereliability"],"title":"What SREs can learn from Aviation industry?","uri":"http://www.anshulpatel.in/posts/sre_aviation_process/"},{"content":"This article discusses the creation of Self-signed CA and TLS certificates using Terrform\nHow TLS Works? Link to heading TLS is an industry standward way to add encryption for data in transist.\nCertificate Authority (CA) is an entity responsible for issuing TLS certificates to websites or services.\nPrivate CA is developed by creating public/private key pair. Public portion is then published which is called CA certificate. Private Key is kept securely else attacker can use the Private Key to issue invalide/malicious TLS certificates.\nTLS certificate is developed by creating another public/private key, the public key is then signed by CA’s private key and this signed public key becomes the TLS certificate. The private key will be used to decrypt the initial message encrypted by the TLS certificate and it should be kept securely.\nWhy Private TLS? Link to heading Generating private CA and TLS certificate is cost-efficient. Cost-efficieny allows to prototype TLS based application on internal environments before releasing or using actual production certificates. If application is privately used in Organization’s infrastructure then Private TLS and CA could be more beneficial. Terraform TLS Module Link to heading Terraform TLS module provides an easy way to create CA and corresponding service certificates.\nGist Link to heading Above example code creates the following:\nca.crt : CA certificate which will be included either in your Browser/OS keychain. api.anveshak.in.crt: Certificate for DNS api.anveshak.in. api.anveshak.in.pem: Private key for DNS api.anveshak.in. www.anveshak.in.cr: Certificate for DNS www.anveshak.in. www.anveshak.in.pem: Private key for DNS www.anveshak.in. Diagram Link to heading ","description":"","tags":["security","infrastructureascode"],"title":"Create Self-Signed CA and TLS certificates using Terraform","uri":"http://www.anshulpatel.in/posts/private_tls_terrform/"},{"content":" My meetup talk discusses about securely distributing docker images in Software Supply Chain using Docker Notary.\nSlides Link to heading Pictures Link to heading ","description":"","tags":["containers","security","devops"],"title":"Docker Notary","uri":"http://www.anshulpatel.in/meetups/notary_sterlite_tech/"},{"content":"This article discusses the subtle differences between fault tolerance, disaster recovery and high availability.\nFault Tolerance Link to heading Fault tolerant system continues to operate either in optimal or degraded state even after facing single or multiple failures. Fault tolerant system strives for zero downtime. Replicating same component is not enough to achieve fault tolerance, rather multiple components make the overall system fault tolerant. High Availability Link to heading Highly available system doesn’t prevent outages. Highly available system does guarantee that outages will be brief, because it will not take much time to redeploy the required component. Generally failover from outage is automatic. Multiple components can be designed to be highly available to make the overall system fault tolerant. Disaster Recovery Link to heading Murphy’s law whatever can go wrong, will go wrong. If failures of system are not addressed, it will eventually lead to system outage. Disaster Recovery is about set of policies and procedures about recovering data from failed infrastructure caused by disruptive events such as power outage, flood or cyberattack. Recovered/Backedup data is generally migrated/moved to new infrastructure. ","description":"","tags":["sitereliability","highavailability"],"title":"Fault Tolerance, High Availability \u0026 Disaster Recovery","uri":"http://www.anshulpatel.in/posts/ft_ha_dr/"},{"content":"Chaos engineering is the discipline of experimenting on a distributed system, in order to build confidence in the system’s capability to withstand turbulent conditions in production.\nFollowing things should be kept in mind while designing a Chaos Experiment:\n1. Pick a Hypothesis Link to heading This step involves the selection of hypothesis which is required to be tested.\nFor eg:\nVerifying that terminating/shutting half of the EC2 instances belonging to Auto-Scaling Group of particular service won’t cause service outage. 2. Identify the metrics to monitor for the experiment Link to heading This steps discusses about the metrics which will enable you to evaluate the outcome of the experiment.\nFor eg:\nTerminating half of the instances in Auto-Scaling group of the service might result in the following scenarios:\n25% increase in response latency of the service.\n30% increase in CPU utilization of existing machines.\n3. Notify the involved Business Units Link to heading This is an important step which discusses about notifying the Service Business Unit so that all the teams around that service are aware of following:\nWhat is that you will be performing on the service? Why are you performing it on the service? When you will be peforming it on the service? 4. Run the experiment Link to heading This step involves to run the chaos experiment and observe the metrics.\nIf you’re running the experiment in the production, ability to abort/stop the experiment could help in preventing unnecessary harm if experiment doesn’t execute as per the plan.\n5. Analyze the results Link to heading In this step, you gather the metrics to answer the following question:\nWas the hypothesis correct? Was the service resilient to the chaos/failure events that were injected/exposed to it? Did anything happen that shouldn’t happen? 6. Automate the process Link to heading Once you’ve confidence in manually running your chaos experiments, automating the same with scripts and workflow engine can help you run the experiments regularly and automatically.\nFamous Chaos Engineering Tools Link to heading Chaos Monkey Blockade Chaos Lambda Chaos Lemur References Link to heading Awesome-Chaos-Engineering ","description":"","tags":["sitereliability","scalability","chaosengineering","highavailability","continuousdelivery"],"title":"Designing a Chaos Engineering Experiment","uri":"http://www.anshulpatel.in/posts/chaos_engg_exprmnt_dsgn/"},{"content":" My lightening talk discusses on structure, architecture, framework and workflow of developing Command Line Interface (CLI) applications using Golang.\nSlides Link to heading Pictures Link to heading ","description":"","tags":["programming","devops"],"title":"Building CLI applications with Golang","uri":"http://www.anshulpatel.in/meetups/lightening_talk_building_cli_golang/"},{"content":"Service Level Indicator(SLI), Service Level Object(SLO) \u0026 Service Level Agreement(SLA) are parameters with which reliability, availability and performance of the service are measured.\nService Level Indicator Link to heading SLI are the parameters which indicates the successful transactions, requests served by the service over the predefined intervals of time. These parameters allows to measure much required performance and availability of the service. Measuring these parameters also enables to improve them gradually.\nKey Examples are:\nAvailability/Uptime of the service. Number of successful transactions/requests. Consistency and durability of the data. Service Level Objective Link to heading SLO defines the acceptable downtime of the service. For multiple components of the service, there can be different parameters which defines the acceptable downtime. It is common pattern to start with low SLO and gradually increase it.\nKey Examples are:\nDurability of disks should be 99.9%. Availability of service should be 99.95% Service should successfully serve 99.999% requests/transactions. Service Level Agreement Link to heading SLA defines the penalty that service provider should pay in an event of service unavailability for pre-defined period of time. Service provider should clearly define the failure factors for which they will be accountable(Domain of responsibility). It is common pattern to have loose SLA than SLO, for instance: SLA is 99% and SLO is 99.5%. If the service is overly available, then SLA/SLO can be used as error budget to deploy complex releases to production.\nKey Examples of Penalty are:\nPartial refund of service subscription fee. Additional subscription time added for free. Insightful References Link to heading Google CRE Life Lessons\n","description":"","tags":["sitereliability"],"title":"SLI, SLO \u0026 SLA","uri":"http://www.anshulpatel.in/posts/sre_sli_sla_slo/"},{"content":"Traditionally, Linux Kernel distinguishes its processes with the following\ntwo categories:\nPrivileged Processes: These processes allow the user to bypass all Kernel permission checks.\nUnprivileged Processes: These processes are subject to full permission checks, such as the effective UID, GID, and supplementary group list.\nGranting full privileged access to a user process might induce system abuse, like unauthorized changes of data, backdoors, changing ACL, etc.\nLinux 2.2 shipped with a solution called Capabilities. Capabilities allows the developer to grant binaries/files specific permissions.\nExample Link to heading Let’s say we want to start a Simple HTTP Server module of Python on port 80 with a non-privileged user. If we try to start the process without granting any capabilities, we will get the following error:\nanshulp@dzone-vagrant-box:$ python -m SimpleHTTPServer 80 Traceback (most recent call last): File \"/usr/lib/python2.7/runpy.py\", line 174, in \\_run_module_as_main \"**main**\", fname, loader, pkg_name) File \"/usr/lib/python2.7/runpy.py\", line 72, in \\_run_code ... File \"/usr/lib/python2.7/socket.py\", line 228, in meth return getattr(self.\\_sock,name)(\\*args) socket.error: [Errno 13] Permission denied Let’s add the CAP_NET_BIND_SERVICE capability to our Python binary.\nsudo setcap 'CAP_NET_BIND_SERVICE+ep' /usr/bin/python2.7 The above command states that we are adding the CAP_NET_BIND_SERVICE capability to our /usr/bin/python2.7 file. +ep indicates that the file is Effective and Permitted ( “-” would remove it).\nNow let’s try to run the Python Simple HTTP Server module again on port 80:\nanshulp@dzone-vagrant-box:$ python -m SimpleHTTPServer 80 Serving HTTP on 0.0.0.0 port 80 ... 172.28.128.1 - - [06/Jul/2017 11:30:13] \"GET / HTTP/1.1\" 200 - 172.28.128.1 - - [06/Jul/2017 11:30:13] code 404, message File not found 172.28.128.1 - - [06/Jul/2017 11:30:13] \"GET /favicon.ico HTTP/1.1\" 404 - 172.28.128.1 - - [06/Jul/2017 11:30:13] code 404, message File not found 172.28.128.1 - - [06/Jul/2017 11:30:13] \"GET /favicon.ico HTTP/1.1\" 404 - We are now able to serve traffic over privileged port 80 with a non-privileged user.\nAt the time of writing this article, there are over 40 capabilities which can be assigned per requirement.\nThere are 3 modes for Capabilities:\ne: Effective - This indicates that the capability is “activated.”\np: Permitted - This indicates that the capability can be used.\ni: Inherited - This indicates that the capability is inherited by child elements/subprocesses.\nCapabilities provide a concise and efficient way to assign privileged permissions to non-privileged users.\n","description":"","tags":["security","linux"],"title":"Linux Kernel Capabilities Explained","uri":"http://www.anshulpatel.in/posts/linux_kernel_capabilities/"},{"content":" My meetup talk discusses about how to build immutable operating systems [Unikernels] with LinuxKit.\nSlides Link to heading Pictures Link to heading ","description":"","tags":["linux","containers","performance","devops"],"title":"LinuxKit","uri":"http://www.anshulpatel.in/meetups/linuxkit_crest_systems/"},{"content":"Linux based systems uses defined terminologies for different components of processors.\nChip: Chip or CPU chip is Integrated Circuit (IC) which encompasses single or multiple cores.\nSockets: Socket is a physical connector on Motherboard that accepts the chip. Motherboards can have multiple sockets.\nCore: A core is basic computation unit of CPU capable of running single program context.\nHyperThreading: Hyperthreading is the capability of a Core to run multiple program context. It makes single core appear logically as multiple cores on the same chip.\nProcessor: Processor can be defined as either single core or multicore chip.\nProcesses: A process is a program running on the computer. It has memory stack associated with it.\nThreads: A thread is a process that doesn’t have memory stack associated with it. A thread is tied to a parent process. Threads can execute simultaneously on separate cores.\n","description":"","tags":["linux","performance"],"title":"Linux CPU Terminologies Explained","uri":"http://www.anshulpatel.in/posts/cpu_terminologies/"},{"content":"HSTS is an opt-in security specified by a web application through the use of a special response header.\nWhen the browser receives this header, the browser prevents all communication over HTTP to that specified domain. Instead the browser will send all communication over HTTPS.\nWhy HSTS? Link to heading HSTS is designed to overcome following threats:\nUser bookmarks or manual types like “http://example.com” are vulnerable to man-in-the-middle attack.\nA web application that is intended to be communicated over HTTPS, contains HTTP links or serves content over HTTP.\nMan-in-the-middle attack which makes victim accept the bad certificate by intercepting network traffic.\nHSTS in Action Link to heading Webserver directive should be added to upgrade all HTTP connections to HTTPS connections on first-time website access.\nWebserver directive to add HSTS header in HTTPS section.\nIf the configuration is successful, you will get following 307(Internal Redirect) redirection from the browser.\n","description":"","tags":["security"],"title":"HTTP Strict Transport Security(HSTS)","uri":"http://www.anshulpatel.in/posts/hsts/"},{"content":"Continuous Integration and Continuous Delivery enabled the frequent release and deployment of software platforms and services.\nSuch frequency of updates and introduction of new Software, have made the failure inevitable.\nWhat is Postmortem Report ? Link to heading Definition from Google SRE Team :\nA postmortem is a written record of an incident, its impact, the actions taken to mitigate or resolve it, the root cause, and follow-up actions to prevent the incident from recurring.\nWhen to create Postmortem Report ? Link to heading Service Downtime.\nService Degradation beyond acceptable threshold.\nData loss.\nMonitoring System Failure.\nRelease Rollback.\nCharacteristics of Postmortem Report Link to heading Reports should be blameless.\nFocus should be on identifying the cause and solution of the incident without pointing out any individual or team.\nReport should not be seen as weakness but as an opportunity to make the Software Architecture more resilient to failure.\nMinimal Postmortem Report Link to heading Issue : R2D2APIGateway down for 5 minutes (Github Issue : #2592)\nDate : 24-12-2016\nAuthors : Anshul Patel\nImpact : 50K API requests lost\nRoot Cause : Increased Traffic triggered kernel OOM.\nResolution :\nUpdated the Machine family from t2.large to m4.xlarge. R2D2APIGateway now has capacity to handle 5X traffic surge. (Github Issue: #2595) Detection : Monitoring System detected timeouts for queries to R2D2APIGateway.\nFollow-Up Actions: Update the machine family in the Infrastructure-as-code scripts to prevent the same in future. (Github Issue: #2598)\n","description":"","tags":["sitereliability"],"title":"Postmortem Culture","uri":"http://www.anshulpatel.in/posts/postmortem/"},{"content":"Software Versioning makes it easy and convenient to track, test, deploy, patch, and rollback features, enhancements and bug/fixes across variety of environmentsin Software Supply Chain.\nSemantic Versioning provides an efficient and effective framework to version the software products, so that the infamous Dependency Hell can be avoided.\nWhat is Semantic Versioning ? Link to heading Semantic Versioning uses MAJOR.MINOR.PATCH number scheme for software products . For Eg: 2.0.5\nHow to decide when to update the version numbers ? Link to heading Increment PATCH number during:\nBackward Compatible Bug Fix. Backward Compatible Maintenance Release. Backward Compatible Hot Fix. Increment MINOR number during:\nBackward Compatible new Framework Adoption. Backward Compatible new Functionality Implementation. Backward Compatible Framework Enhancement. Backward Compatible Functionality Enhancement. Note: PATCH should be reset to 0 when MINOR is incremented.\nIncrement MAJOR number during:\nAny Backward Incompatible change. MAJOR zero (0.Y.Z) should be considered for development. Product shouldn’t be considered stable and shouldn’t be deployed on Production. Note: PATH, MINOR both should be reset to 0 when MAJOR is incremented.\nWhat about versioning during Continuous Integrations (CI) ? Link to heading Pre-release numbers with Semantic Versioning can tackle the problem of versioning during CI.\nFor Eg: X.Y.Z-${CI_BUILD_NUMBER}, 2.0.5-1992\nOnce the software product is ready for prime time, pre-release number should be removed and product should be published/released to external repository.\nNote: Once the package is released/published, it MUST NOT be modified. Any modifications MUST be released as a new version.\n","description":"","tags":["devops"],"title":"Semantic Versioning Simplified","uri":"http://www.anshulpatel.in/posts/semantic_versioning/"},{"content":"2016 saw 71% rise in Denial of Service attacks with biggest attack of 623 Gbps and mega attacks averaging around 100 Gbps.\nIf unprepared, Denial of Service can result in Financial losses, Reputational Damage, Customer Attrition and Legal Pursuits.\nCommon Denial of Service Attacks Link to heading HTTP Flood Attacks Link to heading Volumetric Attack: Simultaneously many machines launch HTTP requests towards a target, saturating it’s resources or saturating bandwidth. Malicious Application Requests: Simultaneous GET/POST/PUT requests to target, which causes high CPU utilization. For Eg: Password Reset request, Long running requests like report execution, downloading large files, requests calling slow hash function SYN Attacks Link to heading Simultaneous TCP requests are sent to target, but doesn’t complete 3-way handshake. For Eg: Initiating request via Spoofed IP. UDP and ICMP Attacks Link to heading Simultaneously flood random ports on target using UDP protocol. Spoofing is much easier as there is no 3-way handshake in UDP. ICMP Attacks Link to heading Simultaneously launching ICMP echo requests to the target with spoofed IP. Simultaneously sending large packets to the target causing buffer-overflow. DNS Attack Reflection and Amplification Link to heading DNS reflection is achieved by eliciting a response from a DNS resolvers to a spoofed IP. Simultaneously sending out DNS query with a spoofed IP to a DNS resolver can overwhelm it. Basic Mitigations Link to heading Establish Traffic reputation Filter Ingress traffic Disable unused ports and block unnecssary protocols Blackholing suspected or malicious traffic Implement Web Application Firewall (WAF) Use third-party services like CloudFlare, Akamai, Incapsula. ","description":"","tags":["sitereliability","security"],"title":"Denial of Service(DoS): Attacks and Mitigation","uri":"http://www.anshulpatel.in/posts/ddos_attack_mitigation/"},{"content":"From Iron Age(Bare Metal) Computing to Cloud Computing, Distributed Systems have played a vital role in processing and delivering information, but we are just getting started.\nWe are set to enter Zetta Byte era with just 40% of total world population connected to Internet.\nSince the inception of Internet, every technology giant is using aspects of Distributed Systems to achieve high availability and scalability in order to cater the ever growing service requests.\nDistributed Systems have provided basis for Software Architectures like\nCORBA, SOA, Microservices.\nInternet of Things which is poised to reach ~ 38 billion devices by 2020, is further expanding and evolving Distributed Systems.\nWhat are Distributed Systems ? Link to heading A Distributed System provides single coherent view of collection of independent computing machines. Multiple software components running on multiple computers, running as single system. Computers can be connected locally or geographically.\nCharacteristics of Distributed Systems Link to heading Concurrency Fault Tolerance Scalability Transperancy Heterogeneity Resource Sharing Fallacies of Distributed Computing Link to heading Reliable Network Zero Latency Infinite Bandwidth Secure Network Static Topology Homogeneous Network Single/Central Administration ","description":"","tags":["highavailability","scalability"],"title":"Distributed Systems: Workhorse of Computing Age","uri":"http://www.anshulpatel.in/posts/distributed_systems/"},{"content":" My meetup talk discusses about the architecture, configuration, scalability and management of Apache Zookeeper.\nSlides Link to heading Pictures Link to heading ","description":"","tags":["bigdata"],"title":"Apache Zookeeper","uri":"http://www.anshulpatel.in/meetups/apache_zookeeper_talentica/"},{"content":" My meetup talk discusses about basics of Docker containers and how to orchestrate the containers at scale using AWS ECS.\nSlides Link to heading Pictures Link to heading ","description":"","tags":["cloud","devops","containers"],"title":"Container Cluster Management with AWS ECS","uri":"http://www.anshulpatel.in/meetups/amazon_ecs_talentica_meetup/"},{"content":"I am passionate about designing robust, efficient, secure, performant, compliant, and thrifty platform to accelerate the application development lifecycle.\nPresent Link to heading Infrastructure Engineer @ Woven Planet , Tokyo Past Link to heading Senior DevOps Lead @ Infostretch (now Apexon) , Ahmedabad Senior DevOps Engineer @ Talentica , Pune Software Configuration Management Engineer @ Amdocs , Pune Junior Systems Engineer @ Elitecore (now Sterlite Tech) , Ahmedabad Certifications Link to heading Google Cloud Certified Professional Cloud Architect CloudBees Certified Jenkins Engineer AWS Certified Solutins Architect - Associate Red Hat Certified Engineer Skillset Link to heading Operating System: Linux, Windows Version Control System: GitHub, GitLab Public Clouds: Amazon Web Services (AWS), Google Cloud Platform (GCP) CI/CD Systems: Concourse, Jenkins, Harness.io, GCP Cloud Build Observability: Prometheus, AWS Cloudwatch, GCP Operations Suite, Elasticsearch Kibana Logstash, Jaeger, Grafana Infrastructure as Code: Terraform, Ansible, Packer, AWS CDK, Pulumi Container Systems: Docker, Kubernetes [GKE, EKS], Sysbox Unikernel Toolkits: LinuxKit Chaos Engineering: Chaos Lambda, Gremlin Platform, Toxiproxy Web Servers and Proxy: Nginx, Apache HTTP Server, Traefik Identity and Access Management: Keycloak, Azure AD, AWS IAM, GCP IAM Database: MySQL, PostgreSQL, MongoDB Programming Languages: Java, Go Scripting Languages: Python, Javascript, Bash API Management: Kong Caching Systems: Redis Big Data: Apache Mesos, Apache Airflow, Apache Zookeeper, Apache Kafka ","description":"","tags":null,"title":"Experience Summary","uri":"http://www.anshulpatel.in/about/"},{"content":" https://github.com/awsdocs/aws-systems-manager-user-guide/pull/125 https://github.com/awsdocs/amazon-ec2-auto-scaling-user-guide/pull/17 https://github.com/tektoncd/pipeline/issues/3721 https://github.com/mongodb/terraform-provider-mongodbatlas/issues/464 https://github.com/avelino/awesome-go/pull/2365 https://github.com/googleapis/google-resumable-media-python/issues/60 https://github.com/hashicorp/terraform-provider-google/issues/1952 https://github.com/edenhill/kcat/issues/73 https://github.com/philips-labs/terraform-aws-github-runner/issues/2287 https://github.com/python/mypy/issues/9223 https://github.com/keycloak/keycloak-admin-ui/issues/915 ","description":"","tags":null,"title":"Opensource Contributions","uri":"http://www.anshulpatel.in/oss/"}]